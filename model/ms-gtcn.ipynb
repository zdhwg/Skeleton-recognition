{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "from model.ms_tcn import MultiScale_TemporalConv as MS_TCN\n",
    "from model.mlp import MLP\n",
    "from model.activation import activation_factory\n",
    "from graph.tools import k_adjacency, normalize_adjacency_matrix\n",
    "\n",
    "\n",
    "class UnfoldTemporalWindows(nn.Module):\n",
    "    def __init__(self, window_size, window_stride, window_dilation=1):\n",
    "        super().__init__()\n",
    "        self.window_size = window_size\n",
    "        self.window_stride = window_stride\n",
    "        self.window_dilation = window_dilation\n",
    "\n",
    "        self.padding = (window_size + (window_size-1) * (window_dilation-1) - 1) // 2 #这个数值一直是0\n",
    "        self.unfold = nn.Unfold(kernel_size=(self.window_size, 1), #（3，1），（5，1）\n",
    "                                dilation=(self.window_dilation, 1), #（1，1）\n",
    "                                stride=(self.window_stride, 1),     #（1，1）\n",
    "                                padding=(self.padding, 0))          #（0，0）\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Input shape: (N,C,T,V)\n",
    "        N, C, T, V = x.shape\n",
    "        #输入数据4维\n",
    "        x = self.unfold(x)\n",
    "        #unfold函数的输入数据是四维，但输出是三维的，沿T维度将window_size大小的T维度拉伸成一维\n",
    "        #输出维度：(N,windowsize*V,C*(T-windowsize+1))\n",
    "        # Permute extra channels from window size to the graph dimension; -1 for number of windows\n",
    "        x = x.view(N, C, self.window_size, -1, V).permute(0,1,3,2,4).contiguous()\n",
    "        x = x.view(N, C, -1, self.window_size * V)#将window_size大小的时间维度上的组成一个\n",
    "        return x\n",
    "\n",
    "\n",
    "class SpatialTemporal_MS_GCN(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_channels,\n",
    "                 out_channels,\n",
    "                 A_binary,\n",
    "                 num_scales,  #A的阶次\n",
    "                 window_size,\n",
    "                 disentangled_agg=True,#邻接矩阵的两种模式，是否分解\n",
    "                 use_Ares=True,#是否使用自适应的Ares矩阵\n",
    "                 residual=False,\n",
    "                 dropout=0,\n",
    "                 activation='relu'):\n",
    "\n",
    "        super().__init__()\n",
    "        self.num_scales = num_scales\n",
    "        self.window_size = window_size\n",
    "        self.use_Ares = use_Ares\n",
    "        A = self.build_spatial_temporal_graph(A_binary, window_size)#在时间轴上拼成一个更大的邻接矩阵，                 #大小为（N*window_size,N*window_size）\n",
    "\n",
    "        if disentangled_agg:\n",
    "            A_scales = [k_adjacency(A, k, with_self=True) for k in range(num_scales)] #with_self=True,\n",
    "            #意味着加上I,但前面已经包括了I?\n",
    "            A_scales = np.concatenate([normalize_adjacency_matrix(g) for g in A_scales]) #正则化\n",
    "            #在列方向上进行拼接，大小为（N*window_size*num_scales,N*window_size)\n",
    "        else: #是先正则化还是先求幂\n",
    "            # Self-loops have already been included in A\n",
    "            A_scales = [normalize_adjacency_matrix(A) for k in range(num_scales)] #num_scales理论上应该的等于windowsize\n",
    "            A_scales = [np.linalg.matrix_power(g, k) for k, g in enumerate(A_scales)]\n",
    "            A_scales = np.concatenate(A_scales)\n",
    "        #上式是用A的多项式来计算\n",
    "        self.A_scales = torch.Tensor(A_scales)#转换成pytorch中的tensor\n",
    "        self.V = len(A_binary)#len是第一个维度上的长度\n",
    "\n",
    "        if use_Ares:#自适应矩阵的参数化\n",
    "            self.A_res = nn.init.uniform_(nn.Parameter(torch.randn(self.A_scales.shape)), -1e-6, 1e-6)\n",
    "        else:\n",
    "            self.A_res = torch.tensor(0)\n",
    "\n",
    "        self.mlp = MLP(in_channels * num_scales, [out_channels], dropout=dropout, activation='linear')\n",
    "\n",
    "        # Residual connection\n",
    "        #残差连接？\n",
    "        if not residual:\n",
    "            self.residual = lambda x: 0#将x投影为0\n",
    "        elif (in_channels == out_channels):\n",
    "            self.residual = lambda x: x#保持不变\n",
    "        else:\n",
    "            self.residual = MLP(in_channels, [out_channels], activation='linear')\n",
    "\n",
    "        self.act = activation_factory(activation)#激活函数\n",
    "    #下面函数是在原来的邻接矩阵上扩大相应的时间步（window-size倍）\n",
    "    def build_spatial_temporal_graph(self, A_binary, window_size):\n",
    "        assert isinstance(A_binary, np.ndarray), 'A_binary should be of type `np.ndarray`'\n",
    "        V = len(A_binary)\n",
    "        V_large = V * window_size#没有作用\n",
    "        A_binary_with_I = A_binary + np.eye(len(A_binary), dtype=A_binary.dtype)\n",
    "        # Build spatial-temporal graph\n",
    "        A_large = np.tile(A_binary_with_I, (window_size, window_size)).copy()\n",
    "        #np.title:沿x轴和y轴复制\n",
    "        #函数作用是创建一个更大的邻接矩阵\n",
    "        return A_large\n",
    "\n",
    "    def forward(self, x):\n",
    "        N, C, T, V = x.shape    # T = number of windows \n",
    "\n",
    "        # Build graphs\n",
    "        A = self.A_scales.to(x.dtype).to(x.device) + self.A_res.to(x.dtype).to(x.device)#将变量放到GPU上\n",
    "        #最终的A,这里的A不是方阵，大小为（N*window_size*num_scales,N*window_size)\n",
    "        # Perform Graph Convolution\n",
    "        res = self.residual(x)\n",
    "        agg = torch.einsum('vu,nctu->nctv', A, x)#爱因斯坦简记法，按维度u求和（节点）\n",
    "        agg = agg.view(N, C, T, self.num_scales, V)#这里是将前面concatenate的矩阵分开?\n",
    "        agg = agg.permute(0,3,1,2,4).contiguous().view(N, self.num_scales*C, T, V)#特征数增加num_scales倍\n",
    "        out = self.mlp(agg)\n",
    "        out += res\n",
    "        return self.act(out)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
